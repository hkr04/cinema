12/08/2024 12:24:33 - INFO - transformers.tokenization_utils_base - loading file vocab.json

12/08/2024 12:24:33 - INFO - transformers.tokenization_utils_base - loading file merges.txt

12/08/2024 12:24:33 - INFO - transformers.tokenization_utils_base - loading file tokenizer.json

12/08/2024 12:24:33 - INFO - transformers.tokenization_utils_base - loading file added_tokens.json

12/08/2024 12:24:33 - INFO - transformers.tokenization_utils_base - loading file special_tokens_map.json

12/08/2024 12:24:33 - INFO - transformers.tokenization_utils_base - loading file tokenizer_config.json

12/08/2024 12:24:33 - WARNING - transformers.tokenization_utils_base - Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

12/08/2024 12:24:33 - INFO - llamafactory.data.loader - Loading dataset influence_identity.json...

12/08/2024 12:24:34 - INFO - llamafactory.data.loader - Loading dataset influence_demo_train.json...

12/08/2024 12:24:34 - INFO - transformers.configuration_utils - loading configuration file ./Qwen2.5-7B-Instruct/config.json

12/08/2024 12:24:34 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "_name_or_path": "./Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 131072,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.38.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}


12/08/2024 12:24:34 - INFO - transformers.modeling_utils - loading weights file ./Qwen2.5-7B-Instruct/model.safetensors.index.json

12/08/2024 12:24:34 - INFO - transformers.modeling_utils - Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.

12/08/2024 12:24:34 - INFO - transformers.generation.configuration_utils - Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "use_cache": false
}


12/08/2024 12:24:38 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing Qwen2ForCausalLM.


12/08/2024 12:24:38 - INFO - transformers.modeling_utils - All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at ./Qwen2.5-7B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.

12/08/2024 12:24:38 - INFO - transformers.generation.configuration_utils - loading configuration file ./Qwen2.5-7B-Instruct/generation_config.json

12/08/2024 12:24:38 - INFO - transformers.generation.configuration_utils - Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}


12/08/2024 12:24:38 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.

12/08/2024 12:24:38 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.

12/08/2024 12:24:38 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.

12/08/2024 12:24:38 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA

12/08/2024 12:24:38 - INFO - llamafactory.model.loader - trainable params: 2523136 || all params: 7618139648 || trainable%: 0.0331

12/08/2024 12:24:38 - WARNING - accelerate.utils.other - Detected kernel version 4.19.24, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.

12/08/2024 12:24:38 - INFO - transformers.trainer - Using auto half precision backend

12/08/2024 12:24:38 - INFO - llamafactory.train.utils - Using LoRA+ optimizer with loraplus lr ratio 8.00.

12/08/2024 12:24:38 - INFO - transformers.trainer - ***** Running training *****

12/08/2024 12:24:38 - INFO - transformers.trainer -   Num examples = 3,002

12/08/2024 12:24:38 - INFO - transformers.trainer -   Num Epochs = 3

12/08/2024 12:24:38 - INFO - transformers.trainer -   Instantaneous batch size per device = 4

12/08/2024 12:24:38 - INFO - transformers.trainer -   Total train batch size (w. parallel, distributed & accumulation) = 8

12/08/2024 12:24:38 - INFO - transformers.trainer -   Gradient Accumulation steps = 2

12/08/2024 12:24:38 - INFO - transformers.trainer -   Total optimization steps = 1,125

12/08/2024 12:24:38 - INFO - transformers.trainer -   Number of trainable parameters = 2,523,136

12/08/2024 12:24:47 - INFO - llamafactory.extras.callbacks - {'loss': 1.8236, 'learning_rate': 9.9995e-05, 'epoch': 0.01}

12/08/2024 12:24:56 - INFO - llamafactory.extras.callbacks - {'loss': 1.6432, 'learning_rate': 9.9981e-05, 'epoch': 0.03}

12/08/2024 12:25:04 - INFO - llamafactory.extras.callbacks - {'loss': 1.6855, 'learning_rate': 9.9956e-05, 'epoch': 0.04}

12/08/2024 12:25:12 - INFO - llamafactory.extras.callbacks - {'loss': 1.5878, 'learning_rate': 9.9922e-05, 'epoch': 0.05}

12/08/2024 12:25:22 - INFO - llamafactory.extras.callbacks - {'loss': 1.5158, 'learning_rate': 9.9878e-05, 'epoch': 0.07}

12/08/2024 12:25:30 - INFO - llamafactory.extras.callbacks - {'loss': 1.4083, 'learning_rate': 9.9825e-05, 'epoch': 0.08}

12/08/2024 12:25:38 - INFO - llamafactory.extras.callbacks - {'loss': 1.3774, 'learning_rate': 9.9761e-05, 'epoch': 0.09}

12/08/2024 12:25:46 - INFO - llamafactory.extras.callbacks - {'loss': 1.4127, 'learning_rate': 9.9688e-05, 'epoch': 0.11}

12/08/2024 12:25:55 - INFO - llamafactory.extras.callbacks - {'loss': 1.3596, 'learning_rate': 9.9606e-05, 'epoch': 0.12}

12/08/2024 12:26:05 - INFO - llamafactory.extras.callbacks - {'loss': 1.4153, 'learning_rate': 9.9513e-05, 'epoch': 0.13}

12/08/2024 12:26:13 - INFO - llamafactory.extras.callbacks - {'loss': 1.3066, 'learning_rate': 9.9411e-05, 'epoch': 0.15}

12/08/2024 12:26:22 - INFO - llamafactory.extras.callbacks - {'loss': 1.3783, 'learning_rate': 9.9300e-05, 'epoch': 0.16}

12/08/2024 12:26:33 - INFO - llamafactory.extras.callbacks - {'loss': 1.2977, 'learning_rate': 9.9179e-05, 'epoch': 0.17}

12/08/2024 12:26:42 - INFO - llamafactory.extras.callbacks - {'loss': 1.2158, 'learning_rate': 9.9048e-05, 'epoch': 0.19}

12/08/2024 12:26:51 - INFO - llamafactory.extras.callbacks - {'loss': 1.3409, 'learning_rate': 9.8907e-05, 'epoch': 0.20}

12/08/2024 12:27:00 - INFO - llamafactory.extras.callbacks - {'loss': 1.2305, 'learning_rate': 9.8757e-05, 'epoch': 0.21}

12/08/2024 12:27:08 - INFO - llamafactory.extras.callbacks - {'loss': 1.2777, 'learning_rate': 9.8598e-05, 'epoch': 0.23}

12/08/2024 12:27:17 - INFO - llamafactory.extras.callbacks - {'loss': 1.2311, 'learning_rate': 9.8429e-05, 'epoch': 0.24}

12/08/2024 12:27:26 - INFO - llamafactory.extras.callbacks - {'loss': 1.2665, 'learning_rate': 9.8251e-05, 'epoch': 0.25}

12/08/2024 12:27:33 - INFO - llamafactory.extras.callbacks - {'loss': 1.4160, 'learning_rate': 9.8063e-05, 'epoch': 0.27}

12/08/2024 12:27:33 - INFO - transformers.trainer - Saving model checkpoint to saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-100

12/08/2024 12:27:34 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-100/tokenizer_config.json

12/08/2024 12:27:34 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-100/special_tokens_map.json

12/08/2024 12:27:43 - INFO - llamafactory.extras.callbacks - {'loss': 1.2587, 'learning_rate': 9.7866e-05, 'epoch': 0.28}

12/08/2024 12:27:51 - INFO - llamafactory.extras.callbacks - {'loss': 1.2533, 'learning_rate': 9.7660e-05, 'epoch': 0.29}

12/08/2024 12:27:59 - INFO - llamafactory.extras.callbacks - {'loss': 1.3293, 'learning_rate': 9.7444e-05, 'epoch': 0.31}

12/08/2024 12:28:08 - INFO - llamafactory.extras.callbacks - {'loss': 1.4175, 'learning_rate': 9.7219e-05, 'epoch': 0.32}

12/08/2024 12:28:17 - INFO - llamafactory.extras.callbacks - {'loss': 1.2266, 'learning_rate': 9.6985e-05, 'epoch': 0.33}

12/08/2024 12:28:26 - INFO - llamafactory.extras.callbacks - {'loss': 1.2824, 'learning_rate': 9.6741e-05, 'epoch': 0.35}

12/08/2024 12:28:35 - INFO - llamafactory.extras.callbacks - {'loss': 1.3297, 'learning_rate': 9.6489e-05, 'epoch': 0.36}

12/08/2024 12:28:43 - INFO - llamafactory.extras.callbacks - {'loss': 1.4074, 'learning_rate': 9.6227e-05, 'epoch': 0.37}

12/08/2024 12:28:52 - INFO - llamafactory.extras.callbacks - {'loss': 1.3054, 'learning_rate': 9.5957e-05, 'epoch': 0.39}

12/08/2024 12:29:01 - INFO - llamafactory.extras.callbacks - {'loss': 1.1742, 'learning_rate': 9.5677e-05, 'epoch': 0.40}

12/08/2024 12:29:10 - INFO - llamafactory.extras.callbacks - {'loss': 1.2589, 'learning_rate': 9.5389e-05, 'epoch': 0.41}

12/08/2024 12:29:18 - INFO - llamafactory.extras.callbacks - {'loss': 1.4164, 'learning_rate': 9.5092e-05, 'epoch': 0.43}

12/08/2024 12:29:27 - INFO - llamafactory.extras.callbacks - {'loss': 1.3265, 'learning_rate': 9.4786e-05, 'epoch': 0.44}

12/08/2024 12:29:36 - INFO - llamafactory.extras.callbacks - {'loss': 1.2864, 'learning_rate': 9.4471e-05, 'epoch': 0.45}

12/08/2024 12:29:46 - INFO - llamafactory.extras.callbacks - {'loss': 1.1971, 'learning_rate': 9.4147e-05, 'epoch': 0.47}

12/08/2024 12:29:55 - INFO - llamafactory.extras.callbacks - {'loss': 1.3345, 'learning_rate': 9.3815e-05, 'epoch': 0.48}

12/08/2024 12:30:04 - INFO - llamafactory.extras.callbacks - {'loss': 1.3131, 'learning_rate': 9.3475e-05, 'epoch': 0.49}

12/08/2024 12:30:13 - INFO - llamafactory.extras.callbacks - {'loss': 1.1818, 'learning_rate': 9.3126e-05, 'epoch': 0.51}

12/08/2024 12:30:22 - INFO - llamafactory.extras.callbacks - {'loss': 1.2243, 'learning_rate': 9.2768e-05, 'epoch': 0.52}

12/08/2024 12:30:31 - INFO - llamafactory.extras.callbacks - {'loss': 1.3346, 'learning_rate': 9.2402e-05, 'epoch': 0.53}

12/08/2024 12:30:31 - INFO - transformers.trainer - Saving model checkpoint to saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-200

12/08/2024 12:30:31 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-200/tokenizer_config.json

12/08/2024 12:30:31 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-200/special_tokens_map.json

12/08/2024 12:30:40 - INFO - llamafactory.extras.callbacks - {'loss': 1.1357, 'learning_rate': 9.2028e-05, 'epoch': 0.55}

12/08/2024 12:30:48 - INFO - llamafactory.extras.callbacks - {'loss': 1.3250, 'learning_rate': 9.1646e-05, 'epoch': 0.56}

12/08/2024 12:30:57 - INFO - llamafactory.extras.callbacks - {'loss': 1.1826, 'learning_rate': 9.1256e-05, 'epoch': 0.57}

12/08/2024 12:31:06 - INFO - llamafactory.extras.callbacks - {'loss': 1.2601, 'learning_rate': 9.0857e-05, 'epoch': 0.59}

12/08/2024 12:31:16 - INFO - llamafactory.extras.callbacks - {'loss': 1.1435, 'learning_rate': 9.0451e-05, 'epoch': 0.60}

12/08/2024 12:31:25 - INFO - llamafactory.extras.callbacks - {'loss': 1.2779, 'learning_rate': 9.0037e-05, 'epoch': 0.61}

12/08/2024 12:31:34 - INFO - llamafactory.extras.callbacks - {'loss': 1.2350, 'learning_rate': 8.9614e-05, 'epoch': 0.63}

12/08/2024 12:31:42 - INFO - llamafactory.extras.callbacks - {'loss': 1.3249, 'learning_rate': 8.9185e-05, 'epoch': 0.64}

12/08/2024 12:31:51 - INFO - llamafactory.extras.callbacks - {'loss': 1.2040, 'learning_rate': 8.8747e-05, 'epoch': 0.65}

12/08/2024 12:32:00 - INFO - llamafactory.extras.callbacks - {'loss': 1.1054, 'learning_rate': 8.8302e-05, 'epoch': 0.67}

12/08/2024 12:32:08 - INFO - llamafactory.extras.callbacks - {'loss': 1.3189, 'learning_rate': 8.7850e-05, 'epoch': 0.68}

12/08/2024 12:32:17 - INFO - llamafactory.extras.callbacks - {'loss': 1.2485, 'learning_rate': 8.7390e-05, 'epoch': 0.69}

12/08/2024 12:32:26 - INFO - llamafactory.extras.callbacks - {'loss': 1.2493, 'learning_rate': 8.6923e-05, 'epoch': 0.71}

12/08/2024 12:32:35 - INFO - llamafactory.extras.callbacks - {'loss': 1.1718, 'learning_rate': 8.6448e-05, 'epoch': 0.72}

12/08/2024 12:32:43 - INFO - llamafactory.extras.callbacks - {'loss': 1.2634, 'learning_rate': 8.5967e-05, 'epoch': 0.73}

12/08/2024 12:32:51 - INFO - llamafactory.extras.callbacks - {'loss': 1.2704, 'learning_rate': 8.5479e-05, 'epoch': 0.75}

12/08/2024 12:33:00 - INFO - llamafactory.extras.callbacks - {'loss': 1.1667, 'learning_rate': 8.4983e-05, 'epoch': 0.76}

12/08/2024 12:33:08 - INFO - llamafactory.extras.callbacks - {'loss': 1.1886, 'learning_rate': 8.4481e-05, 'epoch': 0.77}

12/08/2024 12:33:16 - INFO - llamafactory.extras.callbacks - {'loss': 1.2256, 'learning_rate': 8.3972e-05, 'epoch': 0.79}

12/08/2024 12:33:25 - INFO - llamafactory.extras.callbacks - {'loss': 1.2547, 'learning_rate': 8.3457e-05, 'epoch': 0.80}

12/08/2024 12:33:25 - INFO - transformers.trainer - Saving model checkpoint to saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-300

12/08/2024 12:33:25 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-300/tokenizer_config.json

12/08/2024 12:33:25 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-300/special_tokens_map.json

12/08/2024 12:33:33 - INFO - llamafactory.extras.callbacks - {'loss': 1.1581, 'learning_rate': 8.2934e-05, 'epoch': 0.81}

12/08/2024 12:33:41 - INFO - llamafactory.extras.callbacks - {'loss': 1.2172, 'learning_rate': 8.2406e-05, 'epoch': 0.83}

12/08/2024 12:33:49 - INFO - llamafactory.extras.callbacks - {'loss': 1.2194, 'learning_rate': 8.1871e-05, 'epoch': 0.84}

12/08/2024 12:33:58 - INFO - llamafactory.extras.callbacks - {'loss': 1.1615, 'learning_rate': 8.1330e-05, 'epoch': 0.85}

12/08/2024 12:34:08 - INFO - llamafactory.extras.callbacks - {'loss': 1.1363, 'learning_rate': 8.0783e-05, 'epoch': 0.87}

12/08/2024 12:34:17 - INFO - llamafactory.extras.callbacks - {'loss': 1.0124, 'learning_rate': 8.0230e-05, 'epoch': 0.88}

12/08/2024 12:34:27 - INFO - llamafactory.extras.callbacks - {'loss': 1.2429, 'learning_rate': 7.9671e-05, 'epoch': 0.89}

12/08/2024 12:34:36 - INFO - llamafactory.extras.callbacks - {'loss': 1.1974, 'learning_rate': 7.9106e-05, 'epoch': 0.91}

12/08/2024 12:34:45 - INFO - llamafactory.extras.callbacks - {'loss': 1.1095, 'learning_rate': 7.8536e-05, 'epoch': 0.92}

12/08/2024 12:34:54 - INFO - llamafactory.extras.callbacks - {'loss': 1.2668, 'learning_rate': 7.7960e-05, 'epoch': 0.93}

12/08/2024 12:35:03 - INFO - llamafactory.extras.callbacks - {'loss': 1.1181, 'learning_rate': 7.7378e-05, 'epoch': 0.95}

12/08/2024 12:35:12 - INFO - llamafactory.extras.callbacks - {'loss': 1.2727, 'learning_rate': 7.6791e-05, 'epoch': 0.96}

12/08/2024 12:35:21 - INFO - llamafactory.extras.callbacks - {'loss': 1.2301, 'learning_rate': 7.6199e-05, 'epoch': 0.97}

12/08/2024 12:35:30 - INFO - llamafactory.extras.callbacks - {'loss': 1.2433, 'learning_rate': 7.5602e-05, 'epoch': 0.99}

12/08/2024 12:35:38 - INFO - llamafactory.extras.callbacks - {'loss': 1.2040, 'learning_rate': 7.5000e-05, 'epoch': 1.00}

12/08/2024 12:35:48 - INFO - llamafactory.extras.callbacks - {'loss': 1.2062, 'learning_rate': 7.4393e-05, 'epoch': 1.01}

12/08/2024 12:35:56 - INFO - llamafactory.extras.callbacks - {'loss': 1.1066, 'learning_rate': 7.3781e-05, 'epoch': 1.03}

12/08/2024 12:36:05 - INFO - llamafactory.extras.callbacks - {'loss': 1.0694, 'learning_rate': 7.3165e-05, 'epoch': 1.04}

12/08/2024 12:36:14 - INFO - llamafactory.extras.callbacks - {'loss': 1.1747, 'learning_rate': 7.2544e-05, 'epoch': 1.05}

12/08/2024 12:36:22 - INFO - llamafactory.extras.callbacks - {'loss': 1.1029, 'learning_rate': 7.1919e-05, 'epoch': 1.07}

12/08/2024 12:36:22 - INFO - transformers.trainer - Saving model checkpoint to saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-400

12/08/2024 12:36:22 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-400/tokenizer_config.json

12/08/2024 12:36:22 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-400/special_tokens_map.json

12/08/2024 12:36:31 - INFO - llamafactory.extras.callbacks - {'loss': 1.1936, 'learning_rate': 7.1289e-05, 'epoch': 1.08}

12/08/2024 12:36:39 - INFO - llamafactory.extras.callbacks - {'loss': 0.9981, 'learning_rate': 7.0655e-05, 'epoch': 1.09}

12/08/2024 12:36:48 - INFO - llamafactory.extras.callbacks - {'loss': 1.1719, 'learning_rate': 7.0017e-05, 'epoch': 1.11}

12/08/2024 12:36:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.9930, 'learning_rate': 6.9376e-05, 'epoch': 1.12}

12/08/2024 12:37:06 - INFO - llamafactory.extras.callbacks - {'loss': 1.0899, 'learning_rate': 6.8730e-05, 'epoch': 1.13}

12/08/2024 12:37:14 - INFO - llamafactory.extras.callbacks - {'loss': 1.1614, 'learning_rate': 6.8081e-05, 'epoch': 1.15}

12/08/2024 12:37:24 - INFO - llamafactory.extras.callbacks - {'loss': 1.1521, 'learning_rate': 6.7429e-05, 'epoch': 1.16}

12/08/2024 12:37:33 - INFO - llamafactory.extras.callbacks - {'loss': 1.0519, 'learning_rate': 6.6773e-05, 'epoch': 1.17}

12/08/2024 12:37:41 - INFO - llamafactory.extras.callbacks - {'loss': 1.0777, 'learning_rate': 6.6113e-05, 'epoch': 1.19}

12/08/2024 12:37:50 - INFO - llamafactory.extras.callbacks - {'loss': 1.1166, 'learning_rate': 6.5451e-05, 'epoch': 1.20}

12/08/2024 12:37:59 - INFO - llamafactory.extras.callbacks - {'loss': 1.0107, 'learning_rate': 6.4785e-05, 'epoch': 1.21}

12/08/2024 12:38:08 - INFO - llamafactory.extras.callbacks - {'loss': 1.2339, 'learning_rate': 6.4117e-05, 'epoch': 1.23}

12/08/2024 12:38:16 - INFO - llamafactory.extras.callbacks - {'loss': 1.2680, 'learning_rate': 6.3446e-05, 'epoch': 1.24}

12/08/2024 12:38:25 - INFO - llamafactory.extras.callbacks - {'loss': 1.1055, 'learning_rate': 6.2772e-05, 'epoch': 1.25}

12/08/2024 12:38:33 - INFO - llamafactory.extras.callbacks - {'loss': 1.1932, 'learning_rate': 6.2096e-05, 'epoch': 1.26}

12/08/2024 12:38:42 - INFO - llamafactory.extras.callbacks - {'loss': 1.0680, 'learning_rate': 6.1418e-05, 'epoch': 1.28}

12/08/2024 12:38:51 - INFO - llamafactory.extras.callbacks - {'loss': 1.0219, 'learning_rate': 6.0737e-05, 'epoch': 1.29}

12/08/2024 12:38:59 - INFO - llamafactory.extras.callbacks - {'loss': 1.0522, 'learning_rate': 6.0054e-05, 'epoch': 1.30}

12/08/2024 12:39:08 - INFO - llamafactory.extras.callbacks - {'loss': 1.0200, 'learning_rate': 5.9369e-05, 'epoch': 1.32}

12/08/2024 12:39:19 - INFO - llamafactory.extras.callbacks - {'loss': 1.1073, 'learning_rate': 5.8682e-05, 'epoch': 1.33}

12/08/2024 12:39:19 - INFO - transformers.trainer - Saving model checkpoint to saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-500

12/08/2024 12:39:19 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-500/tokenizer_config.json

12/08/2024 12:39:19 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-500/special_tokens_map.json

12/08/2024 12:39:28 - INFO - llamafactory.extras.callbacks - {'loss': 1.0445, 'learning_rate': 5.7994e-05, 'epoch': 1.34}

12/08/2024 12:39:37 - INFO - llamafactory.extras.callbacks - {'loss': 1.0650, 'learning_rate': 5.7304e-05, 'epoch': 1.36}

12/08/2024 12:39:47 - INFO - llamafactory.extras.callbacks - {'loss': 1.0148, 'learning_rate': 5.6613e-05, 'epoch': 1.37}

12/08/2024 12:39:55 - INFO - llamafactory.extras.callbacks - {'loss': 1.2092, 'learning_rate': 5.5920e-05, 'epoch': 1.38}

12/08/2024 12:40:03 - INFO - llamafactory.extras.callbacks - {'loss': 1.2383, 'learning_rate': 5.5226e-05, 'epoch': 1.40}

12/08/2024 12:40:11 - INFO - llamafactory.extras.callbacks - {'loss': 1.1716, 'learning_rate': 5.4532e-05, 'epoch': 1.41}

12/08/2024 12:40:20 - INFO - llamafactory.extras.callbacks - {'loss': 1.0979, 'learning_rate': 5.3836e-05, 'epoch': 1.42}

12/08/2024 12:40:29 - INFO - llamafactory.extras.callbacks - {'loss': 1.1174, 'learning_rate': 5.3140e-05, 'epoch': 1.44}

12/08/2024 12:40:37 - INFO - llamafactory.extras.callbacks - {'loss': 1.1298, 'learning_rate': 5.2442e-05, 'epoch': 1.45}

12/08/2024 12:40:47 - INFO - llamafactory.extras.callbacks - {'loss': 1.0472, 'learning_rate': 5.1745e-05, 'epoch': 1.46}

12/08/2024 12:40:56 - INFO - llamafactory.extras.callbacks - {'loss': 1.0081, 'learning_rate': 5.1047e-05, 'epoch': 1.48}

12/08/2024 12:41:05 - INFO - llamafactory.extras.callbacks - {'loss': 1.0664, 'learning_rate': 5.0349e-05, 'epoch': 1.49}

12/08/2024 12:41:14 - INFO - llamafactory.extras.callbacks - {'loss': 1.1465, 'learning_rate': 4.9651e-05, 'epoch': 1.50}

12/08/2024 12:41:23 - INFO - llamafactory.extras.callbacks - {'loss': 1.0444, 'learning_rate': 4.8953e-05, 'epoch': 1.52}

12/08/2024 12:41:31 - INFO - llamafactory.extras.callbacks - {'loss': 1.0190, 'learning_rate': 4.8255e-05, 'epoch': 1.53}

12/08/2024 12:41:40 - INFO - llamafactory.extras.callbacks - {'loss': 1.2986, 'learning_rate': 4.7558e-05, 'epoch': 1.54}

12/08/2024 12:41:48 - INFO - llamafactory.extras.callbacks - {'loss': 1.1014, 'learning_rate': 4.6860e-05, 'epoch': 1.56}

12/08/2024 12:41:59 - INFO - llamafactory.extras.callbacks - {'loss': 1.1273, 'learning_rate': 4.6164e-05, 'epoch': 1.57}

12/08/2024 12:42:07 - INFO - llamafactory.extras.callbacks - {'loss': 1.0490, 'learning_rate': 4.5468e-05, 'epoch': 1.58}

12/08/2024 12:42:15 - INFO - llamafactory.extras.callbacks - {'loss': 0.9326, 'learning_rate': 4.4774e-05, 'epoch': 1.60}

12/08/2024 12:42:15 - INFO - transformers.trainer - Saving model checkpoint to saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-600

12/08/2024 12:42:15 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-600/tokenizer_config.json

12/08/2024 12:42:15 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-600/special_tokens_map.json

12/08/2024 12:42:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.9725, 'learning_rate': 4.4080e-05, 'epoch': 1.61}

12/08/2024 12:42:34 - INFO - llamafactory.extras.callbacks - {'loss': 1.1456, 'learning_rate': 4.3387e-05, 'epoch': 1.62}

12/08/2024 12:42:42 - INFO - llamafactory.extras.callbacks - {'loss': 1.0204, 'learning_rate': 4.2696e-05, 'epoch': 1.64}

12/08/2024 12:42:51 - INFO - llamafactory.extras.callbacks - {'loss': 1.1627, 'learning_rate': 4.2006e-05, 'epoch': 1.65}

12/08/2024 12:42:59 - INFO - llamafactory.extras.callbacks - {'loss': 1.0659, 'learning_rate': 4.1318e-05, 'epoch': 1.66}

12/08/2024 12:43:08 - INFO - llamafactory.extras.callbacks - {'loss': 0.9847, 'learning_rate': 4.0631e-05, 'epoch': 1.68}

12/08/2024 12:43:16 - INFO - llamafactory.extras.callbacks - {'loss': 1.0037, 'learning_rate': 3.9946e-05, 'epoch': 1.69}

12/08/2024 12:43:25 - INFO - llamafactory.extras.callbacks - {'loss': 1.1233, 'learning_rate': 3.9263e-05, 'epoch': 1.70}

12/08/2024 12:43:34 - INFO - llamafactory.extras.callbacks - {'loss': 1.0324, 'learning_rate': 3.8582e-05, 'epoch': 1.72}

12/08/2024 12:43:42 - INFO - llamafactory.extras.callbacks - {'loss': 1.0209, 'learning_rate': 3.7904e-05, 'epoch': 1.73}

12/08/2024 12:43:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.9559, 'learning_rate': 3.7228e-05, 'epoch': 1.74}

12/08/2024 12:43:58 - INFO - llamafactory.extras.callbacks - {'loss': 1.0206, 'learning_rate': 3.6554e-05, 'epoch': 1.76}

12/08/2024 12:44:07 - INFO - llamafactory.extras.callbacks - {'loss': 1.1567, 'learning_rate': 3.5883e-05, 'epoch': 1.77}

12/08/2024 12:44:15 - INFO - llamafactory.extras.callbacks - {'loss': 1.0637, 'learning_rate': 3.5215e-05, 'epoch': 1.78}

12/08/2024 12:44:23 - INFO - llamafactory.extras.callbacks - {'loss': 1.0566, 'learning_rate': 3.4549e-05, 'epoch': 1.80}

12/08/2024 12:44:32 - INFO - llamafactory.extras.callbacks - {'loss': 1.0606, 'learning_rate': 3.3887e-05, 'epoch': 1.81}

12/08/2024 12:44:40 - INFO - llamafactory.extras.callbacks - {'loss': 1.0718, 'learning_rate': 3.3227e-05, 'epoch': 1.82}

12/08/2024 12:44:49 - INFO - llamafactory.extras.callbacks - {'loss': 1.1981, 'learning_rate': 3.2571e-05, 'epoch': 1.84}

12/08/2024 12:44:58 - INFO - llamafactory.extras.callbacks - {'loss': 1.0890, 'learning_rate': 3.1919e-05, 'epoch': 1.85}

12/08/2024 12:45:07 - INFO - llamafactory.extras.callbacks - {'loss': 1.0564, 'learning_rate': 3.1270e-05, 'epoch': 1.86}

12/08/2024 12:45:07 - INFO - transformers.trainer - Saving model checkpoint to saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-700

12/08/2024 12:45:07 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-700/tokenizer_config.json

12/08/2024 12:45:07 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-700/special_tokens_map.json

12/08/2024 12:45:16 - INFO - llamafactory.extras.callbacks - {'loss': 1.0234, 'learning_rate': 3.0624e-05, 'epoch': 1.88}

12/08/2024 12:45:25 - INFO - llamafactory.extras.callbacks - {'loss': 1.1673, 'learning_rate': 2.9983e-05, 'epoch': 1.89}

12/08/2024 12:45:33 - INFO - llamafactory.extras.callbacks - {'loss': 1.0082, 'learning_rate': 2.9345e-05, 'epoch': 1.90}

12/08/2024 12:45:42 - INFO - llamafactory.extras.callbacks - {'loss': 1.1736, 'learning_rate': 2.8711e-05, 'epoch': 1.92}

12/08/2024 12:45:50 - INFO - llamafactory.extras.callbacks - {'loss': 1.2478, 'learning_rate': 2.8081e-05, 'epoch': 1.93}

12/08/2024 12:46:00 - INFO - llamafactory.extras.callbacks - {'loss': 0.9741, 'learning_rate': 2.7456e-05, 'epoch': 1.94}

12/08/2024 12:46:10 - INFO - llamafactory.extras.callbacks - {'loss': 1.0428, 'learning_rate': 2.6835e-05, 'epoch': 1.96}

12/08/2024 12:46:18 - INFO - llamafactory.extras.callbacks - {'loss': 1.1352, 'learning_rate': 2.6219e-05, 'epoch': 1.97}

12/08/2024 12:46:27 - INFO - llamafactory.extras.callbacks - {'loss': 1.1687, 'learning_rate': 2.5607e-05, 'epoch': 1.98}

12/08/2024 12:46:35 - INFO - llamafactory.extras.callbacks - {'loss': 1.1821, 'learning_rate': 2.5000e-05, 'epoch': 2.00}

12/08/2024 12:46:44 - INFO - llamafactory.extras.callbacks - {'loss': 1.1029, 'learning_rate': 2.4398e-05, 'epoch': 2.01}

12/08/2024 12:46:53 - INFO - llamafactory.extras.callbacks - {'loss': 0.9864, 'learning_rate': 2.3801e-05, 'epoch': 2.02}

12/08/2024 12:47:01 - INFO - llamafactory.extras.callbacks - {'loss': 1.0001, 'learning_rate': 2.3209e-05, 'epoch': 2.04}

12/08/2024 12:47:10 - INFO - llamafactory.extras.callbacks - {'loss': 0.9502, 'learning_rate': 2.2622e-05, 'epoch': 2.05}

12/08/2024 12:47:17 - INFO - llamafactory.extras.callbacks - {'loss': 0.9782, 'learning_rate': 2.2040e-05, 'epoch': 2.06}

12/08/2024 12:47:26 - INFO - llamafactory.extras.callbacks - {'loss': 0.9449, 'learning_rate': 2.1464e-05, 'epoch': 2.08}

12/08/2024 12:47:34 - INFO - llamafactory.extras.callbacks - {'loss': 1.0659, 'learning_rate': 2.0894e-05, 'epoch': 2.09}

12/08/2024 12:47:44 - INFO - llamafactory.extras.callbacks - {'loss': 1.0936, 'learning_rate': 2.0329e-05, 'epoch': 2.10}

12/08/2024 12:47:54 - INFO - llamafactory.extras.callbacks - {'loss': 1.1921, 'learning_rate': 1.9770e-05, 'epoch': 2.12}

12/08/2024 12:48:03 - INFO - llamafactory.extras.callbacks - {'loss': 0.8247, 'learning_rate': 1.9217e-05, 'epoch': 2.13}

12/08/2024 12:48:03 - INFO - transformers.trainer - Saving model checkpoint to saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-800

12/08/2024 12:48:03 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-800/tokenizer_config.json

12/08/2024 12:48:03 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-800/special_tokens_map.json

12/08/2024 12:48:13 - INFO - llamafactory.extras.callbacks - {'loss': 1.1265, 'learning_rate': 1.8670e-05, 'epoch': 2.14}

12/08/2024 12:48:21 - INFO - llamafactory.extras.callbacks - {'loss': 1.0534, 'learning_rate': 1.8129e-05, 'epoch': 2.16}

12/08/2024 12:48:31 - INFO - llamafactory.extras.callbacks - {'loss': 1.1272, 'learning_rate': 1.7594e-05, 'epoch': 2.17}

12/08/2024 12:48:39 - INFO - llamafactory.extras.callbacks - {'loss': 1.0575, 'learning_rate': 1.7066e-05, 'epoch': 2.18}

12/08/2024 12:48:48 - INFO - llamafactory.extras.callbacks - {'loss': 0.8845, 'learning_rate': 1.6543e-05, 'epoch': 2.20}

12/08/2024 12:48:56 - INFO - llamafactory.extras.callbacks - {'loss': 1.1346, 'learning_rate': 1.6028e-05, 'epoch': 2.21}

12/08/2024 12:49:05 - INFO - llamafactory.extras.callbacks - {'loss': 1.0360, 'learning_rate': 1.5519e-05, 'epoch': 2.22}

12/08/2024 12:49:14 - INFO - llamafactory.extras.callbacks - {'loss': 1.1563, 'learning_rate': 1.5017e-05, 'epoch': 2.24}

12/08/2024 12:49:22 - INFO - llamafactory.extras.callbacks - {'loss': 0.9690, 'learning_rate': 1.4521e-05, 'epoch': 2.25}

12/08/2024 12:49:31 - INFO - llamafactory.extras.callbacks - {'loss': 0.9754, 'learning_rate': 1.4033e-05, 'epoch': 2.26}

12/08/2024 12:49:41 - INFO - llamafactory.extras.callbacks - {'loss': 1.0589, 'learning_rate': 1.3552e-05, 'epoch': 2.28}

12/08/2024 12:49:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.9132, 'learning_rate': 1.3077e-05, 'epoch': 2.29}

12/08/2024 12:49:59 - INFO - llamafactory.extras.callbacks - {'loss': 0.9147, 'learning_rate': 1.2610e-05, 'epoch': 2.30}

12/08/2024 12:50:07 - INFO - llamafactory.extras.callbacks - {'loss': 0.9639, 'learning_rate': 1.2150e-05, 'epoch': 2.32}

12/08/2024 12:50:16 - INFO - llamafactory.extras.callbacks - {'loss': 0.8771, 'learning_rate': 1.1698e-05, 'epoch': 2.33}

12/08/2024 12:50:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.9241, 'learning_rate': 1.1253e-05, 'epoch': 2.34}

12/08/2024 12:50:34 - INFO - llamafactory.extras.callbacks - {'loss': 0.9301, 'learning_rate': 1.0815e-05, 'epoch': 2.36}

12/08/2024 12:50:43 - INFO - llamafactory.extras.callbacks - {'loss': 1.0591, 'learning_rate': 1.0386e-05, 'epoch': 2.37}

12/08/2024 12:50:53 - INFO - llamafactory.extras.callbacks - {'loss': 1.0070, 'learning_rate': 9.9634e-06, 'epoch': 2.38}

12/08/2024 12:51:01 - INFO - llamafactory.extras.callbacks - {'loss': 1.0735, 'learning_rate': 9.5492e-06, 'epoch': 2.40}

12/08/2024 12:51:01 - INFO - transformers.trainer - Saving model checkpoint to saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-900

12/08/2024 12:51:01 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-900/tokenizer_config.json

12/08/2024 12:51:01 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-900/special_tokens_map.json

12/08/2024 12:51:10 - INFO - llamafactory.extras.callbacks - {'loss': 0.9767, 'learning_rate': 9.1428e-06, 'epoch': 2.41}

12/08/2024 12:51:19 - INFO - llamafactory.extras.callbacks - {'loss': 0.9453, 'learning_rate': 8.7443e-06, 'epoch': 2.42}

12/08/2024 12:51:27 - INFO - llamafactory.extras.callbacks - {'loss': 0.8975, 'learning_rate': 8.3539e-06, 'epoch': 2.44}

12/08/2024 12:51:37 - INFO - llamafactory.extras.callbacks - {'loss': 1.0328, 'learning_rate': 7.9717e-06, 'epoch': 2.45}

12/08/2024 12:51:45 - INFO - llamafactory.extras.callbacks - {'loss': 1.0093, 'learning_rate': 7.5976e-06, 'epoch': 2.46}

12/08/2024 12:51:53 - INFO - llamafactory.extras.callbacks - {'loss': 1.0396, 'learning_rate': 7.2318e-06, 'epoch': 2.48}

12/08/2024 12:52:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.9852, 'learning_rate': 6.8743e-06, 'epoch': 2.49}

12/08/2024 12:52:10 - INFO - llamafactory.extras.callbacks - {'loss': 1.1051, 'learning_rate': 6.5253e-06, 'epoch': 2.50}

12/08/2024 12:52:19 - INFO - llamafactory.extras.callbacks - {'loss': 1.0325, 'learning_rate': 6.1847e-06, 'epoch': 2.52}

12/08/2024 12:52:29 - INFO - llamafactory.extras.callbacks - {'loss': 1.0578, 'learning_rate': 5.8526e-06, 'epoch': 2.53}

12/08/2024 12:52:38 - INFO - llamafactory.extras.callbacks - {'loss': 1.0055, 'learning_rate': 5.5292e-06, 'epoch': 2.54}

12/08/2024 12:52:47 - INFO - llamafactory.extras.callbacks - {'loss': 0.9569, 'learning_rate': 5.2144e-06, 'epoch': 2.56}

12/08/2024 12:52:55 - INFO - llamafactory.extras.callbacks - {'loss': 1.1595, 'learning_rate': 4.9084e-06, 'epoch': 2.57}

12/08/2024 12:53:03 - INFO - llamafactory.extras.callbacks - {'loss': 0.9811, 'learning_rate': 4.6111e-06, 'epoch': 2.58}

12/08/2024 12:53:10 - INFO - llamafactory.extras.callbacks - {'loss': 1.0571, 'learning_rate': 4.3227e-06, 'epoch': 2.60}

12/08/2024 12:53:19 - INFO - llamafactory.extras.callbacks - {'loss': 1.0633, 'learning_rate': 4.0432e-06, 'epoch': 2.61}

12/08/2024 12:53:29 - INFO - llamafactory.extras.callbacks - {'loss': 1.0873, 'learning_rate': 3.7727e-06, 'epoch': 2.62}

12/08/2024 12:53:37 - INFO - llamafactory.extras.callbacks - {'loss': 1.0670, 'learning_rate': 3.5112e-06, 'epoch': 2.64}

12/08/2024 12:53:46 - INFO - llamafactory.extras.callbacks - {'loss': 1.2275, 'learning_rate': 3.2587e-06, 'epoch': 2.65}

12/08/2024 12:53:55 - INFO - llamafactory.extras.callbacks - {'loss': 1.0609, 'learning_rate': 3.0154e-06, 'epoch': 2.66}

12/08/2024 12:53:55 - INFO - transformers.trainer - Saving model checkpoint to saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-1000

12/08/2024 12:53:55 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-1000/tokenizer_config.json

12/08/2024 12:53:55 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-1000/special_tokens_map.json

12/08/2024 12:54:05 - INFO - llamafactory.extras.callbacks - {'loss': 0.9568, 'learning_rate': 2.7812e-06, 'epoch': 2.68}

12/08/2024 12:54:14 - INFO - llamafactory.extras.callbacks - {'loss': 1.0403, 'learning_rate': 2.5562e-06, 'epoch': 2.69}

12/08/2024 12:54:23 - INFO - llamafactory.extras.callbacks - {'loss': 1.0317, 'learning_rate': 2.3405e-06, 'epoch': 2.70}

12/08/2024 12:54:32 - INFO - llamafactory.extras.callbacks - {'loss': 0.9690, 'learning_rate': 2.1340e-06, 'epoch': 2.72}

12/08/2024 12:54:41 - INFO - llamafactory.extras.callbacks - {'loss': 0.9972, 'learning_rate': 1.9369e-06, 'epoch': 2.73}

12/08/2024 12:54:50 - INFO - llamafactory.extras.callbacks - {'loss': 1.0463, 'learning_rate': 1.7492e-06, 'epoch': 2.74}

12/08/2024 12:54:58 - INFO - llamafactory.extras.callbacks - {'loss': 0.9974, 'learning_rate': 1.5708e-06, 'epoch': 2.76}

12/08/2024 12:55:07 - INFO - llamafactory.extras.callbacks - {'loss': 1.0841, 'learning_rate': 1.4019e-06, 'epoch': 2.77}

12/08/2024 12:55:16 - INFO - llamafactory.extras.callbacks - {'loss': 1.1211, 'learning_rate': 1.2425e-06, 'epoch': 2.78}

12/08/2024 12:55:24 - INFO - llamafactory.extras.callbacks - {'loss': 1.0602, 'learning_rate': 1.0926e-06, 'epoch': 2.80}

12/08/2024 12:55:34 - INFO - llamafactory.extras.callbacks - {'loss': 1.0315, 'learning_rate': 9.5224e-07, 'epoch': 2.81}

12/08/2024 12:55:42 - INFO - llamafactory.extras.callbacks - {'loss': 1.0543, 'learning_rate': 8.2143e-07, 'epoch': 2.82}

12/08/2024 12:55:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.9970, 'learning_rate': 7.0020e-07, 'epoch': 2.84}

12/08/2024 12:55:58 - INFO - llamafactory.extras.callbacks - {'loss': 0.9962, 'learning_rate': 5.8858e-07, 'epoch': 2.85}

12/08/2024 12:56:06 - INFO - llamafactory.extras.callbacks - {'loss': 1.0552, 'learning_rate': 4.8660e-07, 'epoch': 2.86}

12/08/2024 12:56:15 - INFO - llamafactory.extras.callbacks - {'loss': 1.1288, 'learning_rate': 3.9426e-07, 'epoch': 2.88}

12/08/2024 12:56:23 - INFO - llamafactory.extras.callbacks - {'loss': 1.0283, 'learning_rate': 3.1160e-07, 'epoch': 2.89}

12/08/2024 12:56:32 - INFO - llamafactory.extras.callbacks - {'loss': 1.0384, 'learning_rate': 2.3863e-07, 'epoch': 2.90}

12/08/2024 12:56:41 - INFO - llamafactory.extras.callbacks - {'loss': 1.0089, 'learning_rate': 1.7536e-07, 'epoch': 2.92}

12/08/2024 12:56:49 - INFO - llamafactory.extras.callbacks - {'loss': 1.0050, 'learning_rate': 1.2180e-07, 'epoch': 2.93}

12/08/2024 12:56:49 - INFO - transformers.trainer - Saving model checkpoint to saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-1100

12/08/2024 12:56:49 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-1100/tokenizer_config.json

12/08/2024 12:56:49 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/Custom/lora/train_2024-12-08-12-17-11/tmp-checkpoint-1100/special_tokens_map.json

12/08/2024 12:56:58 - INFO - llamafactory.extras.callbacks - {'loss': 1.0418, 'learning_rate': 7.7962e-08, 'epoch': 2.94}

12/08/2024 12:57:07 - INFO - llamafactory.extras.callbacks - {'loss': 1.0885, 'learning_rate': 4.3858e-08, 'epoch': 2.96}

12/08/2024 12:57:16 - INFO - llamafactory.extras.callbacks - {'loss': 1.0388, 'learning_rate': 1.9494e-08, 'epoch': 2.97}

12/08/2024 12:57:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.9789, 'learning_rate': 4.8738e-09, 'epoch': 2.98}

12/08/2024 12:57:34 - INFO - llamafactory.extras.callbacks - {'loss': 0.9686, 'learning_rate': 0.0000e+00, 'epoch': 3.00}

12/08/2024 12:57:34 - INFO - transformers.trainer - 

Training completed. Do not forget to share your model on huggingface.co/models =)



12/08/2024 12:57:34 - INFO - transformers.trainer - Saving model checkpoint to saves/Custom/lora/train_2024-12-08-12-17-11

12/08/2024 12:57:34 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/Custom/lora/train_2024-12-08-12-17-11/tokenizer_config.json

12/08/2024 12:57:34 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/Custom/lora/train_2024-12-08-12-17-11/special_tokens_map.json

12/08/2024 12:57:35 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.

12/08/2024 12:57:35 - INFO - transformers.modelcard - Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}

